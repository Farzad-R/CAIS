## Generative Adversarial Networks (GANs)

Generative Adversarial Networks (GANs) serve as a powerful framework for training deep learning models to understand and replicate the underlying patterns in the training data. Introduced by Ian Goodfellow in 2014 through the paper "Generative Adversarial Nets," GANs consist of two primary components: a generator and a discriminator. The generator's objective is to produce synthetic images that closely resemble the training images, while the discriminator's role is to distinguish between real training images and fake images generated by the generator. Throughout the training process, the generator continuously improves its ability to create more convincing fakes, while the discriminator aims to enhance its capability to accurately identify the authenticity of the images. The game reaches equilibrium when the generator generates flawless fakes that are indistinguishable from the training data, leaving the discriminator to perpetually guess with 50% confidence whether the output is real or fake.

Theoretically, the ideal outcome of this minimax game is when the generator's output distribution perfectly matches the real data distribution, represented by pg = pdata, and the discriminator makes random guesses regarding the authenticity of the inputs. However, the convergence theory of GANs is an ongoing area of research, and in practice, models don't always achieve this level of training.


# Deep Convolutional Generative Adversarial Network (DCGAN)
A DCGAN (Deep Convolutional Generative Adversarial Network) is an extension of the aforementioned GAN that specifically incorporates convolutional and convolutional-transpose layers within the discriminator and generator respectively. It was initially introduced by Radford et al. in their paper titled "Unsupervised Representation Learning With Deep Convolutional Generative Adversarial Networks". The discriminator consists of strided convolution layers, batch normalization layers, and LeakyReLU activations. It takes a 3x64x64 input image and outputs a scalar probability indicating whether the input belongs to the real data distribution. The generator comprises convolutional-transpose layers, batch normalization layers, and ReLU activations. Its input is a latent vector z, sampled from a standard normal distribution, and it generates a 3x64x64 RGB image. The strided convolution-transpose layers enable the latent vector to be transformed into a volume with the same dimensions as an image. The paper also provides guidelines on optimizer configuration, loss function calculation, and model weight initialization, which will be further detailed in the upcoming sections.

## Dataset
For this tutorial, we will utilize the Celeb-A Faces dataset, which is available for download from this [Link](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html). The dataset will be downloaded in the form of a zip file named img_align_celeba.zip. Once the download is complete, please create a folder named "celeba" and extract the contents of the zip file into that directory. Subsequently, set the input "dataroot" in this notebook to the path of the "celeba" directory you created. Following these steps, the resulting directory structure should appear as follows:
```
/path/to/celeba
    -> img_align_celeba
        -> 188242.jpg
        -> 173822.jpg
        -> 284702.jpg
        -> 537394.jpg
           ...
```


## References:
- [Pytorch examples](https://github.com/pytorch/examples)
- [pytorch tutorials](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html#what-is-a-dcgan)
