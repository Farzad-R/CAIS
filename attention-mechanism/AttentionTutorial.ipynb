{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image was taken from this [reference](https://medium.com/heuritech/attention-mechanism-5aba9a2d4727)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](workshop/attention_tutorial.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](workshop/functions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Example:</b> <br>\n",
    "- I have a batch which contains 64 sentences <br>\n",
    "- Each sentence contains 30 words <br>\n",
    "- Each word is embedded into a vecotor with length 100 => data shape is: (64, 30, 100) <br>\n",
    "- I pass this input to a Bidirectional LSTM with 128 units <br>\n",
    "- My goal is to predict whether this is a positive sentence (0) or a negative sentence (1) => Binary classification <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Layer, Dense, LSTM, Input, Bidirectional, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 2.6.0\n"
     ]
    }
   ],
   "source": [
    "print(\"tensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (64, 30, 100)\n",
      "Output shape (64, 30, 256)\n"
     ]
    }
   ],
   "source": [
    "input = tf.random.normal([64, 30, 100])\n",
    "lstm = Bidirectional(LSTM(128, return_sequences=True))\n",
    "# lstm = LSTM(128, return_sequences=False)\n",
    "\n",
    "lstm_output = lstm(input)\n",
    "\n",
    "print(\"Input shape:\", input.shape)\n",
    "print(\"Output shape\", lstm_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Choices:</b> <br>\n",
    "Feed this output to a Dense layer with 1 neuron to get 0 or 1 for classes, or use attention mechanism.\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Steps:</b>\n",
    "    <ol>\n",
    "     <li>Calculate the score function (dot product)</li>\n",
    "     <li>Normalize the scores</li>\n",
    "     <li>Get the attention weights</li>\n",
    "     <li>Multiply Values with the attention weights</li>\n",
    "     <li>Compute the context vector (summation)</li>  \n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>dot product:</b> <br>\n",
    "    - $\\ a = [1,2,3]$ <br>\n",
    "    - $\\ b = [4,5,6]$ <br>\n",
    "    - $\\ a.b = (a[0]*b[0]) + (a[1]*b[1]) + (a[2]*b[2])$ <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Compute the score function\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight size: (256, 1)\n",
      "attention layer's input size: (64, 30, 256)\n",
      "score shape (64, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_input = lstm_output\n",
    "num_neurons = 1\n",
    "num_dim_perword = attention_input.shape[-1]\n",
    "w = tf.random.normal(shape=(num_dim_perword, num_neurons))\n",
    "score = K.dot(attention_input, w)\n",
    "print(\"weight size:\", w.shape)\n",
    "print(\"attention layer's input size:\", attention_input.shape)\n",
    "print(\"score shape\", score.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Let's add a bias and apply an activation function\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias shape: (30, 1)\n",
      "score shape (64, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "num_words_per_sentence = attention_input.shape[-2]\n",
    "b = tf.random.normal(shape=(num_words_per_sentence, num_neurons))\n",
    "score = score + b\n",
    "# apply tanh\n",
    "score = K.tanh(score)\n",
    "print(\"bias shape:\", b.shape)\n",
    "print(\"score shape\", score.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Why would I want to apply tanh?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30, 1), dtype=float32, numpy=\n",
       "array([[-0.24817254],\n",
       "       [-0.9982055 ],\n",
       "       [-0.98057956],\n",
       "       [-0.977513  ],\n",
       "       [-0.9636431 ],\n",
       "       [-0.99999285],\n",
       "       [-0.74562144],\n",
       "       [-0.815487  ],\n",
       "       [ 0.6806417 ],\n",
       "       [ 0.6527308 ],\n",
       "       [ 0.7553818 ],\n",
       "       [-0.95033544],\n",
       "       [ 0.5429102 ],\n",
       "       [ 0.99938333],\n",
       "       [ 0.05523834],\n",
       "       [ 0.9359733 ],\n",
       "       [ 0.9814705 ],\n",
       "       [ 0.99454844],\n",
       "       [ 0.9992686 ],\n",
       "       [-0.7720548 ],\n",
       "       [ 0.80491376],\n",
       "       [ 0.99999726],\n",
       "       [ 0.9896996 ],\n",
       "       [ 0.9995863 ],\n",
       "       [ 0.9989417 ],\n",
       "       [ 0.83792263],\n",
       "       [ 0.7989326 ],\n",
       "       [-0.63476455],\n",
       "       [ 0.14484772],\n",
       "       [-0.36548555]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Get the attention weights by normalizing the weights (Ex: apply softmax)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_weights shape: (64, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_weights = K.softmax(K.squeeze(score, axis=-1))\n",
    "# attention_weights = K.softmax(score)\n",
    "attention_weights = K.expand_dims(attention_weights, axis=-1)\n",
    "print(\"attention_weights shape:\", attention_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30, 1), dtype=float32, numpy=\n",
       "array([[0.04111318],\n",
       "       [0.01902021],\n",
       "       [0.06929121],\n",
       "       [0.0643251 ],\n",
       "       [0.05613615],\n",
       "       [0.00957971],\n",
       "       [0.01023932],\n",
       "       [0.05810092],\n",
       "       [0.04007023],\n",
       "       [0.00962106],\n",
       "       [0.040673  ],\n",
       "       [0.06840713],\n",
       "       [0.04832774],\n",
       "       [0.01032592],\n",
       "       [0.01433442],\n",
       "       [0.01455746],\n",
       "       [0.06780814],\n",
       "       [0.00963962],\n",
       "       [0.0695413 ],\n",
       "       [0.0388581 ],\n",
       "       [0.01073956],\n",
       "       [0.01159854],\n",
       "       [0.01002639],\n",
       "       [0.00990984],\n",
       "       [0.0101979 ],\n",
       "       [0.01805307],\n",
       "       [0.00957428],\n",
       "       [0.02126205],\n",
       "       [0.06933418],\n",
       "       [0.0693344 ]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Compute the weighted representation of the values\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_input shape: (64, 30, 256)\n",
      "attention_weights shape: (64, 30, 1)\n",
      "Weighted representation of values (keys in this case): (64, 30, 256)\n"
     ]
    }
   ],
   "source": [
    "print(\"attention_input shape:\", attention_input.shape)\n",
    "print(\"attention_weights shape:\", attention_weights.shape)\n",
    "values_weighted_representation = attention_input*attention_weights # here we are using attention input as the values\n",
    "print(\"Weighted representation of values (keys in this case):\", values_weighted_representation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Compute the context vector\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context vector shape: (64, 256)\n"
     ]
    }
   ],
   "source": [
    "context_vector = K.sum(values_weighted_representation, axis=1)\n",
    "print(\"context vector shape:\", context_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Let's write it in a Layer format that can be applied to a model.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer): \n",
    "    def __init__(self, num_neurons=1):    \n",
    "        self.num_neurons = num_neurons\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.num_dim_perword = input_shape[-1]\n",
    "        self.words_pertweet = input_shape[-2]\n",
    "\n",
    "        self.W = self.add_weight(\n",
    "            name=\"att_weight\",\n",
    "            shape=(self.num_dim_perword, self.num_neurons),\n",
    "            initializer='normal')\n",
    "\n",
    "        self.b = self.add_weight(\n",
    "            name=\"att_bias\",\n",
    "            shape=(self.words_pertweet, self.num_neurons),\n",
    "            initializer='zero')\n",
    "        super(Attention, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        e = K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n",
    "        a = K.softmax(e)\n",
    "        a = K.expand_dims(a,axis=-1)\n",
    "        output = x*a\n",
    "        return K.sum(output, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Simple Attention mechanism explained by ChatGPT</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\"\"\"\n",
    "This implementation uses the TensorFlow Layer class to define the attention layer. \n",
    "The build method initializes the weights of the layer (W, b, and u), and the call \n",
    "method applies the attention mechanism to the input. The attention mechanism uses \n",
    "a tanh activation and a softmax activation to calculate the attention scores, and \n",
    "then multiplies the input with the scores to get the weighted input. Finally, the \n",
    "sum of the weighted input is returned.\n",
    "\"\"\"\n",
    "class AttentionLayer(layers.Layer):\n",
    "    def __init__(self, neurons, **kwargs):\n",
    "        self.neurons = neurons\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='W', shape=(input_shape[-1], self.neurons), initializer='uniform', trainable=True)\n",
    "        self.b = self.add_weight(name='b', shape=(self.neurons,), initializer='zeros', trainable=True)\n",
    "        self.u = self.add_weight(name='u', shape=(self.neurons, 1), initializer='uniform', trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        uit = tf.tanh(tf.add(tf.matmul(inputs, self.W), self.b))\n",
    "        ait = tf.nn.softmax(tf.matmul(uit, self.u), axis=-1)\n",
    "        weighted_input = tf.multiply(inputs, ait)\n",
    "        return tf.reduce_sum(weighted_input, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](workshop/attention_two_param.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Explore this class at home and check what each line is doing. <br>\n",
    "    Also think about why? <br>\n",
    "    Apply this layer to a model and see if it can improve the performance.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section is coming from this [reference](https://colab.research.google.com/github/whitead/dmol-book/blob/master/dl/attention.ipynb#scrollTo=tXLxMdk4R9Yu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Example 2:</b> <br>\n",
    "    - We have the following sentence: The sleepy child reads a book <br>\n",
    "    - The goal is to see what parts of the sentence the query (for instance \"book\") should be influenced by. <br>\n",
    "    - Let's embed each word into a vector with length 3 <br>\n",
    "    - consider the query as the word \"book\"\n",
    "    - consider the values to be the sentiment of the word. Is it a positive word (\"happy\") or a negative word (\"angry\")\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Index| Embedding| Word|\n",
    "|:-----|:--------:|----:|\n",
    "| 0    |  0,0,0   | The |\n",
    "| 1    |  2,0,1   | Sleepy |\n",
    "| 2    |  1,-1,-2   | Child |\n",
    "| 3    |  2,3,1   | Reads |\n",
    "| 4    |  -2,0,0   | A |\n",
    "| 5    |  0,2,1   | Book |\n",
    "\n",
    "Keys: $(6, 3)$\n",
    "\\begin{equation}\n",
    "\\mathbf{K} = \\left[\n",
    "\\begin{array}{lccccr}\n",
    "0 & 2 & 1 & 2 & -2 & 0\\\\\n",
    "0 & 0 & -1 & 3 & 0 & 2\\\\\n",
    "0 & 1 & -2 & 1 & 0 & 1\\\\\n",
    "\\end{array}\\right]\n",
    "\\end{equation}\n",
    "Values are the sentiment of each word: <br>\n",
    "Values: $(6, 1)$\n",
    "\\begin{equation}\n",
    "\\mathbf{V} = \\left[ 0, -0.2, 0.3, 0.4, 0, 0.1\\right]\n",
    "\\end{equation}\n",
    "\n",
    "Query: $(3,)$\n",
    "\\begin{equation}\n",
    "\\vec{q} = \\left[0, 2, 1\\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_query = np.random.normal(size=(3,))\n",
    "i_keys = np.random.normal(size=(6, 3))\n",
    "i_values = np.random.normal(size=(6, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>@: dot product</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b> General attenion:</b>\n",
    "</div>\n",
    "\\begin{equation}\n",
    "    \\vec{b} = \\mathrm{softmax}\\left(\\frac{1}{\\sqrt{d}}\\vec{q}\\cdot \\mathbf{K}\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor dot result: [0.09875931 0.27359947 0.10417204 0.07964842 0.22204164 0.22177911]\n",
      "[-0.09801242]\n"
     ]
    }
   ],
   "source": [
    "def softmax(x, axis=None):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=axis)\n",
    "\n",
    "\n",
    "def tensor_dot(q, k):\n",
    "    b = softmax((k @ q) / np.sqrt(q.shape[0]))\n",
    "    return b\n",
    "\n",
    "def attention_layer(q, k, v):\n",
    "    b = tensor_dot(q, k)\n",
    "    print(\"tensor dot result:\", b)\n",
    "    return b @ v\n",
    "i_values = np.random.normal(size=(6, 1))\n",
    "print(attention_layer(i_query, i_keys, i_values))\n",
    "\n",
    "i_values = np.random.normal(size=(6, 2))\n",
    "print(attention_layer(i_query, i_keys, i_values))\n",
    "\n",
    "i_values = np.random.normal(size=(6, 3))\n",
    "print(attention_layer(i_query, i_keys, i_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b> Self attention:</b> when the query, values and keys are equal (We only use the keys)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.37628253, -0.03922664, -0.34993863],\n",
       "       [ 0.48168399,  0.91520147, -1.46493883],\n",
       "       [ 0.7323018 ,  1.00371662,  0.47200163],\n",
       "       [ 0.80631572, -0.36509892, -0.64357495],\n",
       "       [-0.30987736,  0.33676712, -0.16211789],\n",
       "       [-0.23652951,  0.2688794 , -0.25718525]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batched_tensor_dot(q, k):\n",
    "    # a will be batch x seq x feature dim\n",
    "    # which is N x N x 4\n",
    "    # batched dot product in einstein notation\n",
    "    a = np.einsum(\"ij,kj->ik\", q, k) / np.sqrt(q.shape[0])\n",
    "    # now we softmax over sequence\n",
    "    b = softmax(a, axis=1)\n",
    "    return b\n",
    "\n",
    "\n",
    "def self_attention(x):\n",
    "    b = batched_tensor_dot(x, x)\n",
    "    return b @ x\n",
    "\n",
    "\n",
    "attention_result = self_attention(i_keys)\n",
    "print(attention_result.shape)\n",
    "attention_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b> Adding Trainable Parameters:</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.74485375,  1.61535542],\n",
       "       [ 0.16417471, -1.22207086],\n",
       "       [ 4.72068071, -3.76600936],\n",
       "       [-0.40738699,  0.40194374],\n",
       "       [ 1.96006861,  4.9651016 ],\n",
       "       [ 0.57183174, -0.63849133]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weights should be input feature_dim -> desired output feature_dim\n",
    "w_q = np.random.normal(size=(3, 3))\n",
    "w_k = np.random.normal(size=(3, 3))\n",
    "w_v = np.random.normal(size=(3, 2))\n",
    "\n",
    "\n",
    "def trainable_self_attention(x, w_q, w_k, w_v):\n",
    "    q = x @ w_q\n",
    "    k = x @ w_k\n",
    "    v = x @ w_v\n",
    "    b = batched_tensor_dot(q, k)\n",
    "    return b @ v\n",
    "\n",
    "\n",
    "trainable_self_attention(i_keys, w_q, w_k, w_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b> Multi-head:</b> Use an attetnion mechanism multiple times and then concatinate the result\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.42021125e-03, -5.85620417e-02],\n",
       "       [ 5.48249739e-02,  1.42116146e+00],\n",
       "       [ 1.03971379e+00,  2.32066883e+01],\n",
       "       [-1.00868714e-01,  4.20652577e-01],\n",
       "       [-1.47464783e-01, -3.74156949e-01],\n",
       "       [-1.98759090e-03,  7.04947641e-02]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_q_h1 = np.random.normal(size=(3, 3))\n",
    "w_k_h1 = np.random.normal(size=(3, 3))\n",
    "w_v_h1 = np.random.normal(size=(3, 2))\n",
    "w_q_h2 = np.random.normal(size=(3, 3))\n",
    "w_k_h2 = np.random.normal(size=(3, 3))\n",
    "w_v_h2 = np.random.normal(size=(3, 2))\n",
    "w_h = np.random.normal(size=2)\n",
    "\n",
    "\n",
    "def multihead_attention(x, w_q_h1, w_k_h1, w_v_h1, w_q_h2, w_k_h2, w_v_h2):\n",
    "    h1_out = trainable_self_attention(x, w_q_h1, w_k_h1, w_v_h1)\n",
    "    h2_out = trainable_self_attention(x, w_q_h2, w_k_h2, w_v_h2)\n",
    "    # join along last axis so we can use dot.\n",
    "    all_h = np.stack((h1_out, h2_out), -1)\n",
    "    return all_h @ w_h\n",
    "\n",
    "\n",
    "multihead_attention(i_keys, w_q_h1, w_k_h1, w_v_h1, w_q_h2, w_k_h2, w_v_h2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Demo: IMDB sentiment classification.</b> <br>\n",
    "    - Import the imdb dataset that contains classified reviews of the viewers of the movie. \n",
    "</div>\n",
    "\n",
    "[More info about the dataset](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "import numpy as np\n",
    "\n",
    "N_UNIQUE_WORDS = 10000\n",
    "MAXLEN = 200\n",
    "(x_train, y_train),(x_test, y_test) = imdb.load_data(num_words=N_UNIQUE_WORDS)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=MAXLEN)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=MAXLEN)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 200, 128)          1280000   \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,378,945\n",
      "Trainable params: 1,378,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "782/782 [==============================] - 77s 93ms/step - loss: 0.3959 - accuracy: 0.8184\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.2320 - accuracy: 0.9100\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 0.1718 - accuracy: 0.9357\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 73s 94ms/step - loss: 0.1200 - accuracy: 0.9566\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 78s 99ms/step - loss: 0.0815 - accuracy: 0.9712\n",
      "782/782 [==============================] - 22s 27ms/step - loss: 0.4173 - accuracy: 0.8570\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.41733819246292114, 0.8570399880409241]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Benchmark (no attention)\n",
    "model = Sequential()\n",
    "model.add(Embedding(N_UNIQUE_WORDS, 128, input_length=MAXLEN))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x=x_train, y=y_train, batch_size=32, epochs=5)\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 200, 128)          1280000   \n",
      "_________________________________________________________________\n",
      "bidirectional_11 (Bidirectio (None, 200, 128)          98816     \n",
      "_________________________________________________________________\n",
      "attention_8 (Attention)      (None, 128)               328       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,379,273\n",
      "Trainable params: 1,379,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "782/782 [==============================] - 77s 93ms/step - loss: 0.3758 - accuracy: 0.8259\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 74s 94ms/step - loss: 0.2212 - accuracy: 0.9119\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 74s 95ms/step - loss: 0.1627 - accuracy: 0.9392\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 79s 101ms/step - loss: 0.1238 - accuracy: 0.9551\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 73s 93ms/step - loss: 0.0845 - accuracy: 0.9705\n",
      "782/782 [==============================] - 22s 28ms/step - loss: 0.4721 - accuracy: 0.8588\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4720596373081207, 0.8588399887084961]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model with Attention\n",
    "class Attention(Layer): \n",
    "    def __init__(self, num_neurons=1):    \n",
    "        self.num_neurons = num_neurons\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.num_dim_perword = input_shape[-1]\n",
    "        self.words_pertweet = input_shape[-2]\n",
    "\n",
    "        self.W = self.add_weight(\n",
    "            name=\"att_weight\",\n",
    "            shape=(self.num_dim_perword, self.num_neurons),\n",
    "            initializer='normal')\n",
    "\n",
    "        self.b = self.add_weight(\n",
    "            name=\"att_bias\",\n",
    "            shape=(self.words_pertweet, self.num_neurons),\n",
    "            initializer='zero')\n",
    "        super(Attention, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        e = K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n",
    "        a = K.softmax(e)\n",
    "        a = K.expand_dims(a,axis=-1)\n",
    "        output = x*a\n",
    "        return K.sum(output, axis=1)\n",
    "    \n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(N_UNIQUE_WORDS, 128, input_length=MAXLEN))\n",
    "model2.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model2.add(Attention()) # receive 3D and output 3D\n",
    "model2.add(Dense(1, activation='sigmoid'))\n",
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "model2.summary()\n",
    "\n",
    "history = model2.fit(x=x_train, y=y_train, batch_size=32, epochs=5)\n",
    "model2.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ieso_env_v9",
   "language": "python",
   "name": "ieso_env_v9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8e97fb63bcd85d6cbcd650175f604004b3c4ca4ba6b0c093e51f599ce6b801e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
