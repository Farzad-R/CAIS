{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image was taken from this [reference](https://medium.com/heuritech/attention-mechanism-5aba9a2d4727)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![image](workshop/attention_tutorial.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Example:</b> <br>\n",
    "- I have a batch which contains 64 sentences <br>\n",
    "- Each sentence contains 30 words <br>\n",
    "- Each word is embedded into a vecotor with length 100 => data shape is: (64, 30, 100) <br>\n",
    "- I pass this input to a Bidirectional LSTM with 128 units <br>\n",
    "- My goal is to predict whether this is a posivie sentence (0) or a negative sentence (1) => Binary classification <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras import Model\n",
    "from keras.layers import Layer, Dense, LSTM, Input, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 2.6.0\n"
     ]
    }
   ],
   "source": [
    "print(\"tensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (64, 30, 100)\n",
      "Output shape (64, 30, 256)\n"
     ]
    }
   ],
   "source": [
    "input = tf.random.normal([64, 30, 100])\n",
    "lstm = Bidirectional(LSTM(128, return_sequences=True))\n",
    "lstm_output = lstm(input)\n",
    "\n",
    "print(\"Input shape:\", input.shape)\n",
    "print(\"Output shape\", lstm_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Choices:</b> <br>\n",
    "Feed this output to a Dense layer with 1 neuron to get 0 or 1 for classes, or use attention mechanism.\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Steps:</b>\n",
    "    <ol>\n",
    "     <li>Calculate the score function (dot product)</li>\n",
    "     <li>Normalize the scores</li>\n",
    "     <li>Get the attention weights</li>\n",
    "     <li>Multiply Values with the attention weights</li>\n",
    "     <li>Compute the context vector (summation)</li>  \n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>dot product:</b> <br>\n",
    "    - $\\ a = [1,2,3]$ <br>\n",
    "    - $\\ b = [4,5,6]$ <br>\n",
    "    - $\\ a.b = (a[0]*b[0]) + (a[1]*b[1]) + (a[2]*b[2])$ <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Compute the score function\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight size: (256, 1)\n",
      "attention layer's input size: (64, 30, 256)\n",
      "score shape (64, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_input = lstm_output\n",
    "num_neurons = 1\n",
    "num_dim_perword = attention_input.shape[-1]\n",
    "w = tf.random.normal(shape=(num_dim_perword, num_neurons))\n",
    "score = K.dot(attention_input, w)\n",
    "print(\"weight size:\", w.shape)\n",
    "print(\"attention layer's input size:\", attention_input.shape)\n",
    "print(\"score shape\", score.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Let's add a bias and apply an activation function\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias shape: (30, 1)\n",
      "score shape (64, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "num_words_per_sentence = attention_input.shape[-2]\n",
    "b = tf.random.normal(shape=(num_words_per_sentence, num_neurons))\n",
    "score = score + b\n",
    "# apply tanh\n",
    "score = K.tanh(score)\n",
    "print(\"bias shape:\", b.shape)\n",
    "print(\"score shape\", score.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Why would I want to apply tanh?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30, 1), dtype=float32, numpy=\n",
       "array([[ 0.00671984],\n",
       "       [-0.79233164],\n",
       "       [ 0.84262574],\n",
       "       [ 0.9393658 ],\n",
       "       [ 0.96200097],\n",
       "       [ 0.9514732 ],\n",
       "       [-0.7956847 ],\n",
       "       [-0.9320937 ],\n",
       "       [-0.8195906 ],\n",
       "       [-0.9541469 ],\n",
       "       [ 0.5583366 ],\n",
       "       [ 0.66266966],\n",
       "       [ 0.3721411 ],\n",
       "       [-0.95057094],\n",
       "       [ 0.94560724],\n",
       "       [-0.99997675],\n",
       "       [-0.9090281 ],\n",
       "       [-0.99658227],\n",
       "       [ 0.48647708],\n",
       "       [ 0.99285144],\n",
       "       [ 0.9977037 ],\n",
       "       [-0.8550069 ],\n",
       "       [ 0.37468526],\n",
       "       [ 0.98339844],\n",
       "       [ 0.14576234],\n",
       "       [ 0.75789636],\n",
       "       [ 0.9262583 ],\n",
       "       [ 0.7487535 ],\n",
       "       [-0.5285568 ],\n",
       "       [ 0.9952256 ]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Get the attention weights by normalizing the weights (Ex: apply softmax)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_weights shape: (64, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_weights = K.softmax(K.squeeze(score, axis=-1))\n",
    "attention_weights = K.expand_dims(attention_weights, axis=-1)\n",
    "print(\"attention_weights shape:\", attention_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30, 1), dtype=float32, numpy=\n",
       "array([[0.02223728],\n",
       "       [0.01000134],\n",
       "       [0.05129925],\n",
       "       [0.05650992],\n",
       "       [0.05780362],\n",
       "       [0.05719826],\n",
       "       [0.00996786],\n",
       "       [0.00869681],\n",
       "       [0.00973239],\n",
       "       [0.00850712],\n",
       "       [0.0386052 ],\n",
       "       [0.04285062],\n",
       "       [0.03204661],\n",
       "       [0.0085376 ],\n",
       "       [0.05686372],\n",
       "       [0.00812604],\n",
       "       [0.00889974],\n",
       "       [0.00815367],\n",
       "       [0.03592838],\n",
       "       [0.05961467],\n",
       "       [0.05990464],\n",
       "       [0.00939374],\n",
       "       [0.03212824],\n",
       "       [0.05905379],\n",
       "       [0.02555448],\n",
       "       [0.04713174],\n",
       "       [0.05577405],\n",
       "       [0.04670278],\n",
       "       [0.01302009],\n",
       "       [0.05975638]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Compute the weighted representation of the values\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_input shape: (64, 30, 256)\n",
      "attention_weights shape: (64, 30, 1)\n",
      "Weighted representation of values (keys in this case): (64, 30, 256)\n"
     ]
    }
   ],
   "source": [
    "print(\"attention_input shape:\", attention_input.shape)\n",
    "print(\"attention_weights shape:\", attention_weights.shape)\n",
    "values_weighted_representation = attention_input*attention_weights # here we are using attention input as the values\n",
    "print(\"Weighted representation of values (keys in this case):\", values_weighted_representation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Compute the context vector\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context vector shape: (64, 256)\n"
     ]
    }
   ],
   "source": [
    "context_vector = K.sum(values_weighted_representation, axis=1)\n",
    "print(\"context vector shape:\", context_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>In one catch:</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we can pass the context vector to the next layer\n"
     ]
    }
   ],
   "source": [
    "attention_input = lstm_output\n",
    "# initialize the learning parameters\n",
    "w = tf.random.normal(shape=(256, 1))\n",
    "b = tf.random.normal(shape=(30, 1))\n",
    "# compute the score\n",
    "score = K.dot(attention_input, w)\n",
    "score = score + b\n",
    "# Get the attention weights\n",
    "attention_weights = K.softmax(score)\n",
    "# Compute the weighted reepresentation of the values\n",
    "values_weighted_representation = attention_input*attention_weights # here we are using attention_input as the values\n",
    "# Compute the context vector\n",
    "context_vector = K.sum(values_weighted_representation, axis=1)\n",
    "print(\"Now we can pass the context vector to the next layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Let's write it in a Layer format that can be applied to a model.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 30, 100)]         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 30, 256)           234496    \n",
      "_________________________________________________________________\n",
      "attention (Attention)        (None, 256)               286       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 235,039\n",
      "Trainable params: 235,039\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class Attention(Layer): \n",
    "    def __init__(self, num_neurons=1):    \n",
    "        self.num_neurons = num_neurons\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.num_dim_perword = input_shape[-1]\n",
    "        self.words_pertweet = input_shape[-2]\n",
    "\n",
    "        self.W = self.add_weight(\n",
    "            name=\"att_weight\",\n",
    "            shape=(self.num_dim_perword, self.num_neurons),\n",
    "            initializer='normal')\n",
    "\n",
    "        self.b = self.add_weight(\n",
    "            name=\"att_bias\",\n",
    "            shape=(self.words_pertweet, self.num_neurons),\n",
    "            initializer='zero')\n",
    "        super(Attention, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        e = K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n",
    "        a = K.softmax(e)\n",
    "        a = K.expand_dims(a,axis=-1)\n",
    "        output = x*a\n",
    "        return K.sum(output, axis=1)\n",
    "    \n",
    "input_layer = Input(shape=(30, 100))\n",
    "lstm = Bidirectional(LSTM(units=128, return_sequences=True))(input_layer)\n",
    "attn = Attention()(lstm)\n",
    "output = Dense(units=1, activation=\"linear\")(attn)\n",
    "model = Model([input_layer], [output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Simple Attention mechanism explained by ChatGPT</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\"\"\"\n",
    "This implementation uses the TensorFlow Layer class to define the attention layer. \n",
    "The build method initializes the weights of the layer (W, b, and u), and the call \n",
    "method applies the attention mechanism to the input. The attention mechanism uses \n",
    "a tanh activation and a softmax activation to calculate the attention scores, and \n",
    "then multiplies the input with the scores to get the weighted input. Finally, the \n",
    "sum of the weighted input is returned.\n",
    "\"\"\"\n",
    "class AttentionLayer(layers.Layer):\n",
    "    def __init__(self, neurons, **kwargs):\n",
    "        self.neurons = neurons\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='W', shape=(input_shape[-1], self.neurons), initializer='uniform', trainable=True)\n",
    "        self.b = self.add_weight(name='b', shape=(self.neurons,), initializer='zeros', trainable=True)\n",
    "        self.u = self.add_weight(name='u', shape=(self.neurons, 1), initializer='uniform', trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        uit = tf.tanh(tf.add(tf.matmul(inputs, self.W), self.b))\n",
    "        ait = tf.nn.softmax(tf.matmul(uit, self.u), axis=-1)\n",
    "        weighted_input = tf.multiply(inputs, ait)\n",
    "        return tf.reduce_sum(weighted_input, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Explore this class at home and check what each line is doing. <br>\n",
    "    Also think about why? <br>\n",
    "    Apply this layer to a model and see if it can improve the performance.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section is coming from this [reference](https://colab.research.google.com/github/whitead/dmol-book/blob/master/dl/attention.ipynb#scrollTo=tXLxMdk4R9Yu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Example 2:</b> <br>\n",
    "    - We have the following sentence: The sleepy child reads a book <br>\n",
    "    - The goal is to see what parts of the sentence the query (for instance \"book\") should be influenced by. <br>\n",
    "    - Let's embed each word into a vector with length 3 <br>\n",
    "    - consider the query as the word \"book\"\n",
    "    - consider the values to be the sentiment of the word. Is it a positive word (\"happy\") or a negative word (\"angry\")\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Index| Embedding| Word|\n",
    "|:-----|:--------:|----:|\n",
    "| 0    |  0,0,0   | The |\n",
    "| 1    |  2,0,1   | Sleepy |\n",
    "| 2    |  1,-1,-2   | Child |\n",
    "| 3    |  2,3,1   | Reads |\n",
    "| 4    |  -2,0,0   | A |\n",
    "| 5    |  0,2,1   | Book |\n",
    "\n",
    "Keys: $(6, 3)$\n",
    "\\begin{equation}\n",
    "\\mathbf{K} = \\left[\n",
    "\\begin{array}{lccccr}\n",
    "0 & 2 & 1 & 2 & -2 & 0\\\\\n",
    "0 & 0 & -1 & 3 & 0 & 2\\\\\n",
    "0 & 1 & -2 & 1 & 0 & 1\\\\\n",
    "\\end{array}\\right]\n",
    "\\end{equation}\n",
    "\n",
    "Values: $(6, 1)$\n",
    "\\begin{equation}\n",
    "\\mathbf{V} = \\left[ 0, -0.2, 0.3, 0.4, 0, 0.1\\right]\n",
    "\\end{equation}\n",
    "\n",
    "Query: $(3,)$\n",
    "\\begin{equation}\n",
    "\\vec{q} = \\left[0, 2, 1\\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_query = np.random.normal(size=(3,))\n",
    "i_keys = np.random.normal(size=(6, 3))\n",
    "i_values = np.random.normal(size=(6, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>@: dot product</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b> General attenion:</b>\n",
    "</div>\n",
    "\\begin{equation}\n",
    "    \\vec{b} = \\mathrm{softmax}\\left(\\frac{1}{\\sqrt{d}}\\vec{q}\\cdot \\mathbf{K}\\right)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.11157827])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(x, axis=None):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=axis)\n",
    "\n",
    "\n",
    "def tensor_dot(q, k):\n",
    "    b = softmax((k @ q) / np.sqrt(q.shape[0]))\n",
    "    return b\n",
    "\n",
    "def attention_layer(q, k, v):\n",
    "    b = tensor_dot(q, k)\n",
    "    return b @ v\n",
    "\n",
    "attention_layer(i_query, i_keys, i_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b> Self attention:</b> when the query, values and keys are equal (We only use the keys)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.20009993,  0.30845095,  0.19241971],\n",
       "       [ 0.75329141,  0.33190861, -0.17081802],\n",
       "       [ 2.12516013,  0.48464586, -0.8761569 ],\n",
       "       [-0.78917967, -2.29579892, -1.50165363],\n",
       "       [-0.07414448,  0.94402292,  0.78108597],\n",
       "       [ 0.53707919,  0.30445758, -0.01764354]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batched_tensor_dot(q, k):\n",
    "    # a will be batch x seq x feature dim\n",
    "    # which is N x N x 4\n",
    "    # batched dot product in einstein notation\n",
    "    a = np.einsum(\"ij,kj->ik\", q, k) / np.sqrt(q.shape[0])\n",
    "    # now we softmax over sequence\n",
    "    b = softmax(a, axis=1)\n",
    "    return b\n",
    "\n",
    "\n",
    "def self_attention(x):\n",
    "    b = batched_tensor_dot(x, x)\n",
    "    return b @ x\n",
    "\n",
    "\n",
    "attention_result = self_attention(i_keys)\n",
    "print(attention_result.shape)\n",
    "attention_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b> Adding Trainable Parameters:</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.74485375,  1.61535542],\n",
       "       [ 0.16417471, -1.22207086],\n",
       "       [ 4.72068071, -3.76600936],\n",
       "       [-0.40738699,  0.40194374],\n",
       "       [ 1.96006861,  4.9651016 ],\n",
       "       [ 0.57183174, -0.63849133]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# weights should be input feature_dim -> desired output feature_dim\n",
    "w_q = np.random.normal(size=(3, 3))\n",
    "w_k = np.random.normal(size=(3, 3))\n",
    "w_v = np.random.normal(size=(3, 2))\n",
    "\n",
    "\n",
    "def trainable_self_attention(x, w_q, w_k, w_v):\n",
    "    q = x @ w_q\n",
    "    k = x @ w_k\n",
    "    v = x @ w_v\n",
    "    b = batched_tensor_dot(q, k)\n",
    "    return b @ v\n",
    "\n",
    "\n",
    "trainable_self_attention(i_keys, w_q, w_k, w_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b> Multi-head:</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-8.42021125e-03, -5.85620417e-02],\n",
       "       [ 5.48249739e-02,  1.42116146e+00],\n",
       "       [ 1.03971379e+00,  2.32066883e+01],\n",
       "       [-1.00868714e-01,  4.20652577e-01],\n",
       "       [-1.47464783e-01, -3.74156949e-01],\n",
       "       [-1.98759090e-03,  7.04947641e-02]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_q_h1 = np.random.normal(size=(3, 3))\n",
    "w_k_h1 = np.random.normal(size=(3, 3))\n",
    "w_v_h1 = np.random.normal(size=(3, 2))\n",
    "w_q_h2 = np.random.normal(size=(3, 3))\n",
    "w_k_h2 = np.random.normal(size=(3, 3))\n",
    "w_v_h2 = np.random.normal(size=(3, 2))\n",
    "w_h = np.random.normal(size=2)\n",
    "\n",
    "\n",
    "def multihead_attention(x, w_q_h1, w_k_h1, w_v_h1, w_q_h2, w_k_h2, w_v_h2):\n",
    "    h1_out = trainable_self_attention(x, w_q_h1, w_k_h1, w_v_h1)\n",
    "    h2_out = trainable_self_attention(x, w_q_h2, w_k_h2, w_v_h2)\n",
    "    # join along last axis so we can use dot.\n",
    "    all_h = np.stack((h1_out, h2_out), -1)\n",
    "    return all_h @ w_h\n",
    "\n",
    "\n",
    "multihead_attention(i_keys, w_q_h1, w_k_h1, w_v_h1, w_q_h2, w_k_h2, w_v_h2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ieso_env_v9",
   "language": "python",
   "name": "ieso_env_v9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8e97fb63bcd85d6cbcd650175f604004b3c4ca4ba6b0c093e51f599ce6b801e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
