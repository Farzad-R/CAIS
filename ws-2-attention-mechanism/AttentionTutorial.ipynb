{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[reference](https://medium.com/heuritech/attention-mechanism-5aba9a2d4727)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![image](workshop/attention_tutorial.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Example:</b> <br>\n",
    "- I have a batch which contains 64 sentences <br>\n",
    "- Each sentence contains 30 words <br>\n",
    "- Each word is embedded into a vecotor with length 100 => data shape is: (64, 30, 100) <br>\n",
    "- I pass this input to a Bidirectional LSTM with 128 units <br>\n",
    "- My goal is to predict whether this is a posivie sentence (0) or a negative sentence (1) => Binary classification <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras import Model\n",
    "from keras.layers import Layer, Dense, LSTM, Input, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 2.6.0\n"
     ]
    }
   ],
   "source": [
    "print(\"tensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (64, 30, 100)\n",
      "Output shape (64, 30, 256)\n"
     ]
    }
   ],
   "source": [
    "input = tf.random.normal([64, 30, 100])\n",
    "lstm = Bidirectional(LSTM(128, return_sequences=True))\n",
    "lstm_output = lstm(input)\n",
    "\n",
    "print(\"Input shape:\", input.shape)\n",
    "print(\"Output shape\", lstm_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Choices:</b> <br>\n",
    "Feed this output to a Dense layer with 1 neuron to get 0 or 1 for classes, or use attention mechanism.\n",
    "<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Steps:</b>\n",
    "    <ol>\n",
    "     <li>Calculate the score function (dot product)</li>\n",
    "     <li>Normalize the scores</li>\n",
    "     <li>Get the attention weights</li>\n",
    "     <li>Multiply Values with the attention weights</li>\n",
    "     <li>Compute the context vector (summation)</li>  \n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>dot product:</b> <br>\n",
    "    - $\\ a = [1,2,3]$ <br>\n",
    "    - $\\ b = [4,5,6]$ <br>\n",
    "    - $\\ a.b = (a[0]*b[0]) + (a[1]*b[1]) + (a[2]*b[2])$ <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Compute the score function\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight size: (256, 1)\n",
      "attention layer's input size: (64, 30, 256)\n",
      "score shape (64, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_input = lstm_output\n",
    "num_neurons = 1\n",
    "num_dim_perword = attention_input.shape[-1]\n",
    "w = tf.random.normal(shape=(num_dim_perword, num_neurons))\n",
    "score = K.dot(attention_input, w)\n",
    "print(\"weight size:\", w.shape)\n",
    "print(\"attention layer's input size:\", attention_input.shape)\n",
    "print(\"score shape\", score.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Let's add a bias and apply an activation function\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias shape: (30, 1)\n",
      "score shape (64, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "num_words_per_sentence = attention_input.shape[-2]\n",
    "b = tf.random.normal(shape=(num_words_per_sentence, num_neurons))\n",
    "score = score + b\n",
    "# apply tanh\n",
    "score = K.tanh(score)\n",
    "print(\"bias shape:\", b.shape)\n",
    "print(\"score shape\", score.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Why would I want to apply tanh?</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30, 1), dtype=float32, numpy=\n",
       "array([[ 0.00671984],\n",
       "       [-0.79233164],\n",
       "       [ 0.84262574],\n",
       "       [ 0.9393658 ],\n",
       "       [ 0.96200097],\n",
       "       [ 0.9514732 ],\n",
       "       [-0.7956847 ],\n",
       "       [-0.9320937 ],\n",
       "       [-0.8195906 ],\n",
       "       [-0.9541469 ],\n",
       "       [ 0.5583366 ],\n",
       "       [ 0.66266966],\n",
       "       [ 0.3721411 ],\n",
       "       [-0.95057094],\n",
       "       [ 0.94560724],\n",
       "       [-0.99997675],\n",
       "       [-0.9090281 ],\n",
       "       [-0.99658227],\n",
       "       [ 0.48647708],\n",
       "       [ 0.99285144],\n",
       "       [ 0.9977037 ],\n",
       "       [-0.8550069 ],\n",
       "       [ 0.37468526],\n",
       "       [ 0.98339844],\n",
       "       [ 0.14576234],\n",
       "       [ 0.75789636],\n",
       "       [ 0.9262583 ],\n",
       "       [ 0.7487535 ],\n",
       "       [-0.5285568 ],\n",
       "       [ 0.9952256 ]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    Get the attention weights by normalizing the weights (Ex: apply softmax)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_weights shape: (64, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_weights = K.softmax(K.squeeze(score, axis=-1))\n",
    "attention_weights = K.expand_dims(attention_weights, axis=-1)\n",
    "print(\"attention_weights shape:\", attention_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30, 1), dtype=float32, numpy=\n",
       "array([[0.02223728],\n",
       "       [0.01000134],\n",
       "       [0.05129925],\n",
       "       [0.05650992],\n",
       "       [0.05780362],\n",
       "       [0.05719826],\n",
       "       [0.00996786],\n",
       "       [0.00869681],\n",
       "       [0.00973239],\n",
       "       [0.00850712],\n",
       "       [0.0386052 ],\n",
       "       [0.04285062],\n",
       "       [0.03204661],\n",
       "       [0.0085376 ],\n",
       "       [0.05686372],\n",
       "       [0.00812604],\n",
       "       [0.00889974],\n",
       "       [0.00815367],\n",
       "       [0.03592838],\n",
       "       [0.05961467],\n",
       "       [0.05990464],\n",
       "       [0.00939374],\n",
       "       [0.03212824],\n",
       "       [0.05905379],\n",
       "       [0.02555448],\n",
       "       [0.04713174],\n",
       "       [0.05577405],\n",
       "       [0.04670278],\n",
       "       [0.01302009],\n",
       "       [0.05975638]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Compute the weighted reepresentation of the values\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_input shape: (64, 30, 256)\n",
      "attention_weights shape: (64, 30, 1)\n",
      "Weighted representation of values (keys in this case): (64, 30, 256)\n"
     ]
    }
   ],
   "source": [
    "print(\"attention_input shape:\", attention_input.shape)\n",
    "print(\"attention_weights shape:\", attention_weights.shape)\n",
    "values_weighted_representation = attention_input*attention_weights # here we are using attention input as the values\n",
    "print(\"Weighted representation of values (keys in this case):\", values_weighted_representation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Compute the context vector\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context vector shape: (64, 256)\n"
     ]
    }
   ],
   "source": [
    "context_vector = K.sum(values_weighted_representation, axis=1)\n",
    "print(\"context vector shape:\", context_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>In one catch:</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we can pass the context vector to the next layer\n"
     ]
    }
   ],
   "source": [
    "attention_input = lstm_output\n",
    "# initialize the learning parameters\n",
    "w = tf.random.normal(shape=(256, 1))\n",
    "b = tf.random.normal(shape=(30, 1))\n",
    "# compute the score\n",
    "score = K.dot(attention_input, w)\n",
    "score = score + b\n",
    "# Get the attention weights\n",
    "attention_weights = K.softmax(score)\n",
    "# Compute the weighted reepresentation of the values\n",
    "values_weighted_representation = attention_input*attention_weights # here we are using attention_input as the values\n",
    "# Compute the context vector\n",
    "context_vector = K.sum(values_weighted_representation, axis=1)\n",
    "print(\"Now we can pass the context vector to the next layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Let's write it in a Layer format that can be applied to a model.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 30, 100)]         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 30, 256)           234496    \n",
      "_________________________________________________________________\n",
      "attention (Attention)        (None, 256)               286       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 235,039\n",
      "Trainable params: 235,039\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class Attention(Layer): \n",
    "    def __init__(self, num_neurons=1):    \n",
    "        self.num_neurons = num_neurons\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.num_dim_perword = input_shape[-1]\n",
    "        self.words_pertweet = input_shape[-2]\n",
    "\n",
    "        self.W = self.add_weight(\n",
    "            name=\"att_weight\",\n",
    "            shape=(self.num_dim_perword, self.num_neurons),\n",
    "            initializer='normal')\n",
    "\n",
    "        self.b = self.add_weight(\n",
    "            name=\"att_bias\",\n",
    "            shape=(self.words_pertweet, self.num_neurons),\n",
    "            initializer='zero')\n",
    "        super(Attention, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        e = K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n",
    "        a = K.softmax(e)\n",
    "        a = K.expand_dims(a,axis=-1)\n",
    "        output = x*a\n",
    "        return K.sum(output, axis=1)\n",
    "    \n",
    "input_layer = Input(shape=(30, 100))\n",
    "lstm = Bidirectional(LSTM(units=128, return_sequences=True))(input_layer)\n",
    "attn = Attention()(lstm)\n",
    "output = Dense(units=1, activation=\"linear\")(attn)\n",
    "model = Model([input_layer], [output])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Simple Attention mechanism explained by ChatGPT</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\"\"\"\n",
    "This implementation uses the TensorFlow Layer class to define the attention layer. \n",
    "The build method initializes the weights of the layer (W, b, and u), and the call \n",
    "method applies the attention mechanism to the input. The attention mechanism uses \n",
    "a tanh activation and a softmax activation to calculate the attention scores, and \n",
    "then multiplies the input with the scores to get the weighted input. Finally, the \n",
    "sum of the weighted input is returned.\n",
    "\"\"\"\n",
    "class AttentionLayer(layers.Layer):\n",
    "    def __init__(self, neurons, **kwargs):\n",
    "        self.neurons = neurons\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='W', shape=(input_shape[-1], self.neurons), initializer='uniform', trainable=True)\n",
    "        self.b = self.add_weight(name='b', shape=(self.neurons,), initializer='zeros', trainable=True)\n",
    "        self.u = self.add_weight(name='u', shape=(self.neurons, 1), initializer='uniform', trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        uit = tf.tanh(tf.add(tf.matmul(inputs, self.W), self.b))\n",
    "        ait = tf.nn.softmax(tf.matmul(uit, self.u), axis=-1)\n",
    "        weighted_input = tf.multiply(inputs, ait)\n",
    "        return tf.reduce_sum(weighted_input, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Explore this class at home and check what each line is doing. <br>\n",
    "    Also think about why? <br>\n",
    "    Apply this layer to a model and see if it can improve the performance.</b>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "8e97fb63bcd85d6cbcd650175f604004b3c4ca4ba6b0c093e51f599ce6b801e8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
